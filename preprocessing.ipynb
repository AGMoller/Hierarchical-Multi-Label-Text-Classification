{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/icd_coding.json', 'r') as f:\n",
    "    icd_coding = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2818"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{k: v} for k, v in icd_coding[0].items()]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, val_ = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_, test_ = train_test_split(val_, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "Article = TypedDict(\n",
    "    \"Article\",\n",
    "    {\n",
    "        \"id_\": str,\n",
    "        \"title\": str,\n",
    "        \"description\": str,\n",
    "        \"body\": str,\n",
    "        \"chapters\": List[str],\n",
    "        \"blocks\": List[str],\n",
    "        \"categories\": List[str],\n",
    "    },\n",
    ")\n",
    "\n",
    "def get_relevant_data(data: List[Dict]) -> List[Article]:\n",
    "\n",
    "    relevant_data: List[Article] = []\n",
    "    for article in data:\n",
    "\n",
    "        id_, article_data = article.popitem()\n",
    "\n",
    "        title_of_first_paragraph: str = list(article_data[\"texts\"].keys())[0]\n",
    "\n",
    "        relevant_data.append(\n",
    "            Article(\n",
    "                id_=id_,\n",
    "                title=article_data[\"MetaTags\"][\"title\"],\n",
    "                description=article_data[\"MetaTags\"][\"description\"],\n",
    "                body=article_data[\"texts\"][title_of_first_paragraph],\n",
    "                chapters=article_data[\"MetaTags\"][\"ICD_details\"][\"chapters\"],\n",
    "                blocks=article_data[\"MetaTags\"][\"ICD_details\"][\"blocks\"],\n",
    "            )\n",
    "        )\n",
    "    return relevant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ICD codes to int from train\n",
    "train = get_relevant_data(train_)\n",
    "val = get_relevant_data(val_)\n",
    "test = get_relevant_data(test_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chapters = set()\n",
    "all_blocks = set()\n",
    "all_categories = set()\n",
    "\n",
    "for article in train:\n",
    "    for chapter in article[\"chapters\"]:\n",
    "        all_chapters.add(chapter)\n",
    "    for block in article[\"blocks\"]:\n",
    "        all_blocks.add(block)\n",
    "\n",
    "for article in val:\n",
    "    for chapter in article[\"chapters\"]:\n",
    "        all_chapters.add(chapter)\n",
    "    for block in article[\"blocks\"]:\n",
    "        all_blocks.add(block)\n",
    "\n",
    "for article in test:\n",
    "    for chapter in article[\"chapters\"]:\n",
    "        all_chapters.add(chapter)\n",
    "    for block in article[\"blocks\"]:\n",
    "        all_blocks.add(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 199 0\n"
     ]
    }
   ],
   "source": [
    "# Assert the the order is always the same\n",
    "all_chapters_sorted = sorted(all_chapters)\n",
    "all_blocks_sorted = sorted(all_blocks)\n",
    "all_categories_sorted = sorted(all_categories)\n",
    "\n",
    "print(len(all_chapters_sorted), len(all_blocks_sorted), len(all_categories_sorted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map code to int and back\n",
    "int_to_chapter = {idx: code for idx, code in enumerate(all_chapters_sorted)}\n",
    "chapter_to_int = {code: idx for idx, code in int_to_chapter.items()}\n",
    "int_to_block = {idx: code for idx, code in enumerate(all_blocks_sorted)}\n",
    "block_to_int = {code: idx for idx, code in int_to_block.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "train_processed = []\n",
    "\n",
    "for article in train:\n",
    "\n",
    "    title = article[\"title\"]\n",
    "    description = article[\"description\"]\n",
    "    body = article[\"body\"]\n",
    "\n",
    "    text = description + '. ' + body\n",
    "\n",
    "    title_doc = nlp(title)\n",
    "    text_doc = nlp(text)\n",
    "\n",
    "    article_json = {\n",
    "        \"id\": article[\"id_\"],\n",
    "        \"title\": [],\n",
    "        \"chapters\": [],\n",
    "        \"blocks\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    title_tokens = []\n",
    "    for token in title_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        title_tokens.append(token.text.lower())\n",
    "    \n",
    "    text_tokens = []\n",
    "    for token in text_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        text_tokens.append(token.text.lower())\n",
    "\n",
    "    chapters_converted = [chapter_to_int[chapter] for chapter in article[\"chapters\"]]\n",
    "    blocks_converted = [block_to_int[block] for block in article[\"blocks\"]]\n",
    "\n",
    "    labels = []\n",
    "    for chapter in chapters_converted:\n",
    "        labels.append(chapter)\n",
    "    for block in blocks_converted:\n",
    "        labels.append(block + len(int_to_chapter.keys()))\n",
    "\n",
    "    article_json[\"title\"] = title_tokens\n",
    "    article_json[\"text\"] = text_tokens\n",
    "    article_json[\"chapters\"] = chapters_converted\n",
    "    article_json[\"blocks\"] = blocks_converted\n",
    "    article_json[\"labels\"] = labels\n",
    "\n",
    "    train_processed.append(article_json)\n",
    "\n",
    "val_processed = []\n",
    "for article in val:\n",
    "    \n",
    "    title = article[\"title\"]\n",
    "    description = article[\"description\"]\n",
    "    body = article[\"body\"]\n",
    "\n",
    "    text = description + '. ' + body\n",
    "\n",
    "    title_doc = nlp(title)\n",
    "    text_doc = nlp(text)\n",
    "\n",
    "    article_json = {\n",
    "        \"id\": article[\"id_\"],\n",
    "        \"title\": [],\n",
    "        \"chapters\": [],\n",
    "        \"blocks\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    title_tokens = []\n",
    "    for token in title_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        title_tokens.append(token.text.lower())\n",
    "    \n",
    "    text_tokens = []\n",
    "    for token in text_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        text_tokens.append(token.text.lower())\n",
    "\n",
    "    chapters_converted = [chapter_to_int[chapter] for chapter in article[\"chapters\"]]\n",
    "    blocks_converted = [block_to_int[block] for block in article[\"blocks\"]]\n",
    "\n",
    "    labels = []\n",
    "    for chapter in chapters_converted:\n",
    "        labels.append(chapter)\n",
    "    for block in blocks_converted:\n",
    "        labels.append(block + len(int_to_chapter.keys()))\n",
    "\n",
    "    article_json[\"title\"] = title_tokens\n",
    "    article_json[\"text\"] = text_tokens\n",
    "    article_json[\"chapters\"] = chapters_converted\n",
    "    article_json[\"blocks\"] = blocks_converted\n",
    "    article_json[\"labels\"] = labels\n",
    "\n",
    "    val_processed.append(article_json)\n",
    "\n",
    "test_processed = []\n",
    "for article in test:\n",
    "        \n",
    "    title = article[\"title\"]\n",
    "    description = article[\"description\"]\n",
    "    body = article[\"body\"]\n",
    "\n",
    "    text = description + '. ' + body\n",
    "\n",
    "    title_doc = nlp(title)\n",
    "    text_doc = nlp(text)\n",
    "\n",
    "    article_json = {\n",
    "        \"id\": article[\"id_\"],\n",
    "        \"title\": [],\n",
    "        \"text\": [],\n",
    "        \"chapters\": [],\n",
    "        \"blocks\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    title_tokens = []\n",
    "    for token in title_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        title_tokens.append(token.text.lower())\n",
    "    \n",
    "    text_tokens = []\n",
    "    for token in text_doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        text_tokens.append(token.text.lower())\n",
    "\n",
    "    chapters_converted = [chapter_to_int[chapter] for chapter in article[\"chapters\"]]\n",
    "    blocks_converted = [block_to_int[block] for block in article[\"blocks\"]]\n",
    "\n",
    "    labels = []\n",
    "    for chapter in chapters_converted:\n",
    "        labels.append(chapter)\n",
    "    for block in blocks_converted:\n",
    "        labels.append(block + len(int_to_chapter.keys()))\n",
    "\n",
    "    article_json[\"title\"] = title_tokens\n",
    "    article_json[\"text\"] = text_tokens\n",
    "    article_json[\"chapters\"] = chapters_converted\n",
    "    article_json[\"blocks\"] = blocks_converted\n",
    "    article_json[\"labels\"] = labels\n",
    "\n",
    "    test_processed.append(article_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make jsonl file\n",
    "with open(\"data/train.json\", \"w\") as f:\n",
    "    for article in train_processed:\n",
    "        f.write(json.dumps(article) + \"\\n\")\n",
    "\n",
    "# make jsonl file\n",
    "with open(\"data/val.json\", \"w\") as f:\n",
    "    for article in val_processed:\n",
    "        f.write(json.dumps(article) + \"\\n\")\n",
    "\n",
    "# make jsonl file\n",
    "with open(\"data/test.json\", \"w\") as f:\n",
    "    for article in test_processed:\n",
    "        f.write(json.dumps(article) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f50f3cf29a42233610af9fa877c606c3bf2a15d85938ffc270948c1debeb286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
